{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d490d-21f3-47af-b86b-7d9c9e78690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.object = object\n",
    "np.int = int\n",
    "np.float = float\n",
    "np.bool = bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded5d84-b287-4429-bebe-b640d33721ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e90eb80-3e41-4c48-aa0b-ea46a75ced38",
   "metadata": {},
   "source": [
    "Simple Rnn() ->Recurrent nerual network\n",
    "Embedding()\n",
    "LSTM() -> lONG Short Term Memory\n",
    "GRU() -> Gated Recurrent Unit\n",
    "Bidirectional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384e303-5c27-4bc2-b6b8-7c9c8ff94997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a03b00-d2c0-4520-bac0-bab6703b9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('D:\\\\data.csv\\\\data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f045f-6b96-49cb-8b2b-980b2ef4fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0569af9-9c22-4656-b418-5733495544e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): #---------- defult cleaning function to use any time ...\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]',' ',text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\.S+',' ',text)\n",
    "    text = re.sub('<.*?>+',' ',text)\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = re.sub('[^\\w]',' ',text)\n",
    "    text = re.sub('\\w*\\d\\w*',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced3969-ff87-4205-85ac-d5d1af7d5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.english = df.english.map(clean_text)\n",
    "df.spanish = df.spanish.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca28ed-d3a5-4c67-b314-c869b113a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end(text):\n",
    "    text = f'<start> {text} <end>'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7485d-f08e-4850-9e59-b3850c581edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.english = df.english.map(add_start_end)\n",
    "df.spanish = df.spanish.map(add_start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121c1b9-05f9-4607-9963-27ada087e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b9ba9-38ed-44a2-ab14-9d5fab853491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor , lang_tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2756632-bd42-44a3-8665-0f3f7cc3492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sequence , eng_tokenizer = tokenize(df.english)\n",
    "spn_sequence , spn_tokenizer = tokenize(df.spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a40f0-27f8-401f-a028-84afc7953c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test , y_train , y_test = model_selection.train_test_split(eng_sequence,spn_sequence,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb34d1f-a5da-4d09-9649-785855f9d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert (lang,tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print('%d---> %s' % (t,lang.index_word[t]))\n",
    "\n",
    "convert(eng_tokenizer,x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8cb211-658c-4445-89d7-5c2ee14c3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inp_size = len(eng_tokenizer.word_index)+1\n",
    "vocab_tar_size =  len(spn_tokenizer.word_index)+1\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "batch_size=32\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed45b02-ce4c-4c6b-8aaa-903e03cb1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x,y,batch_size=32):\n",
    "    data = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    data = data.shuffle(1028)\n",
    "    data = data.batch(batch_size,drop_remainder = True)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7db0b9-cc66-401c-bad4-ee7997c90207",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(x_train,y_train)\n",
    "test_dataset = create_dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53a1f1-5a87-4cf2-8089-4accbd09885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng , spn in train_dataset.take(1):\n",
    "    print(f'English : {eng.shape}\\n{eng}')\n",
    "    print(f'Spanish : {spn.shape}\\n{spn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66379042-cf5a-4953-83b6-e8f1851a1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "      super(Encoder, self).__init__()\n",
    "\n",
    "      self.batch_size = batch_size\n",
    "      self.encoder_units = encoder_units\n",
    "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
    "      self.gru = tf.keras.layers.GRU(self.encoder_units, \n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116cf88-4d95-4eea-b5b3-8011eaa87fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.batch_size = batch_size\n",
    "      self.decoder_units = decoder_units\n",
    "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
    "      self.gru = tf.keras.layers.GRU(self.decoder_units, \n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           recurrent_initializer = 'glorot_uniform')\n",
    "      \n",
    "      self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, hidden = self.gru(x, initial_state = hidden)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x =  tf.nn.softmax(self.fc(output))\n",
    "    return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7541fa-8abd-474a-acc5-a28b1199a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_inp_size = len(eng_tokenizer.word_index)+1\n",
    "# vocab_tar_size =  len(spn_tokenizer.word_index)+1\n",
    "# embedding_dim = 256\n",
    "# units = 1024\n",
    "# batch_size=32\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(eng, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac3cbb-180c-46e9-8dad-7d14807d568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((batch_size, 1)), sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch size, vocab_size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbb27f-77c8-44a5-9d6f-eb7854d9065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the optimizer using the Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# create the loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "# define the loss function for the training\n",
    "def loss_function(real, pred):\n",
    "  # create the mask to ignore the padding tokens\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  # mask shape == (batch_size, sequence_length)\n",
    "  # calculate the loss\n",
    "  loss_ = loss_object(real, pred)\n",
    "  # mask the loss\n",
    "  # how the mask works:\n",
    "  # if the value is 1, the loss is calculated\n",
    "  # if the value is 0, the loss is ignored\n",
    "    #[1,1,1,1,1,1,0,0,0,0,0] mask\n",
    "    # *\n",
    "    #[2,6,2,1,6,3,2,1,5,7,9] input\n",
    "    # =\n",
    "    #[2,6,2,1,6,3,0,0,0,0,0] output\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  # mask shape == (batch_size, sequence_length)\n",
    "\n",
    "  loss_ *= mask\n",
    "  # calculate the average loss per batch \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bdd9d3-7bdd-4e96-a72d-4993ab217fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training metric \n",
    "train_loss = tf.metrics.Mean(name='train loss')\n",
    "# create the testing metric \n",
    "test_loss =tf.metrics.Mean(name='test loss')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8089b6-401c-4787-af90-49b77abee847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training step\n",
    "# using the tf.function decorator to speed up the training process by converting the training function to a TensorFlow graph\n",
    "@tf.function\n",
    "# define the training step \n",
    "def train_step(inputs, target, enc_hidden):\n",
    "  # the encoder_hidden is the initial hidden state of the encoder\n",
    "  # enc_hidden shape == (batch_size, hidden_size)\n",
    "\n",
    "  # inilaize the loss to zero\n",
    "  loss = 0\n",
    "  # create the gradient tape to record the gradient of the loss with respect to the weights\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    # pass the input to the encoder\n",
    "    # enc_output shape == (batch_size, 49, hidden_size)\n",
    "    # enc_hidden shape == (batch_size, hidden_size)\n",
    "    # using the encoder to get the encoder_output and the encoder_hidden\n",
    "    # using the encoder_hidden as the initial hidden state of the decoder\n",
    "    enc_output, enc_hidden = encoder(inputs, enc_hidden)\n",
    "    # set the initial decoder hidden state to the encoder hidden state\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    # create the start token \n",
    "    # start_token shape == (batch_size, 1)\n",
    "    # repeat the start token for the batch size times\n",
    "    dec_input = tf.expand_dims([spn_tokenizer.word_index['<start>']] * inputs.shape[0], 1)\n",
    "    \n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    \n",
    "    for t in range(1, target.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "      # calculate the loss for the current time step using the loss function\n",
    "      loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(target[:, t], 1)\n",
    "  # calculate the loss for the current batch\n",
    "  batch_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  # get the trainable variables\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  # calculate the gradients using the tape \n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  # update the trainable variables\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  # add the loss to the training loss metric\n",
    "  train_loss(batch_loss)\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20128e-0483-4638-ab4d-a000eaf234dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training step\n",
    "# using the tf.function decorator to speed up the training process by converting the training function to a TensorFlow graph\n",
    "@tf.function \n",
    "def test_step(inputs, target, enc_hidden):\n",
    "    # the encoder_hidden is the initial hidden state of the encoder\n",
    "    # enc_hidden shape == (batch_size, hidden_size)\n",
    "    # inilaize the loss to zero\n",
    "    loss = 0\n",
    "    # pass the input to the encoder \n",
    "    # enc_output shape == (batch_size, 49, hidden_size) \n",
    "    # enc_hidden shape == (batch_size, hidden_size)\n",
    "    # using the encoder to get the encoder_output and the encoder_hidden\n",
    "    enc_output, enc_hidden = encoder(inputs, enc_hidden)\n",
    "    # set the initial decoder hidden state to the encoder hidden state\n",
    "    dec_hidden = enc_hidden\n",
    "    # create the start token\n",
    "    # start_token shape == (batch_size, 1)\n",
    "    # repeat the start token for the batch size times\n",
    "    dec_input = tf.expand_dims([spn_tokenizer.word_index['<start>']] * inputs.shape[0], 1)\n",
    "    for t in range(1, target.shape[1]):\n",
    "        # passing enc_output to the decoder with dec_hidden as the initial hidden state\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "        # calculate the loss for the current time step using the loss function \n",
    "        loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(target[:, t], 1)\n",
    "    # calculate the loss for the current batch\n",
    "    batch_loss = (loss / int(target.shape[1]))\n",
    "    # add the batch loss to the test loss metric\n",
    "    test_loss(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39236a84-1159-4164-b3c7-38cb931a776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the epochs to 3 \n",
    "EPOCHS = 3\n",
    "# set the old test loss to high number \n",
    "old_test_loss=1000000\n",
    "# create the training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # reset the training loss metric\n",
    "    train_loss.reset_states()\n",
    "    # reset the testing loss metric\n",
    "    test_loss.reset_states()\n",
    "\n",
    "    # initalize the hidden state of the encoder to zeros \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    # create the training progress bar set the total number of batches to the length of the training dataset and the batch size to the test size\n",
    "    steps_per_epoch = eng_sequence.shape[0]//batch_size #=> 3717 batch in the dataset \n",
    "    bar = tf.keras.utils.Progbar(target=steps_per_epoch)\n",
    "    \n",
    "    count=0\n",
    "    # iterate over the training dataset \n",
    "    for (batch, (inputs, target)) in enumerate(train_dataset):\n",
    "        # update the progress bar\n",
    "        count += 1\n",
    "        # run the training step\n",
    "        batch_loss = train_step(inputs, target, enc_hidden)\n",
    "        bar.update(count)  # manually update the progress bar\n",
    "                                                  \n",
    "    \n",
    "         \n",
    "    \n",
    "    # iterate over the testing dataset    \n",
    "    for (batch, (inputs, target)) in enumerate(test_dataset):\n",
    "        count += 1\n",
    "        # run the testing step\n",
    "        batch_loss = test_step(inputs, target, enc_hidden)\n",
    "        bar.update(count)\n",
    "    # save the best performance model on the test dataset \n",
    "    \n",
    "    if old_test_loss> test_loss.result():\n",
    "        # set the old test loss to the test loss \n",
    "        old_test_loss= test_loss.result()\n",
    "        encoder.save(filepath='D:\\\\encoder')\n",
    "        decoder.save(filepath='D:\\\\decoder')\n",
    "        print('Model is saved')\n",
    "    # print the training and testing loss\n",
    "    print('#' * 50)\n",
    "    print(f'Epoch #{epoch + 1}')\n",
    "    print(f'Training Loss {train_loss.result()}')\n",
    "    print(f'Testing Loss {test_loss.result()}')\n",
    "    print('#' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66d77c-9ff9-4082-aa4f-bc56709e608c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7f535-034a-45f1-91aa-ea1fe9812401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
